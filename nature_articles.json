[
    {
        "URL": "https://www.nature.com/articles/d41586-024-03958-2",
        "title": "Stress can dull our capacity for joy: mouse brain patterns hint at why",
        "content": [
            "*Illustration_of_nerve_cells_from_the_cerebral_cortex_of_the_brain,_shown_in_blue",
            "Communication between neurons (illustration) in two separate brain regions is patchy in mice that are susceptible to severe stress. Credit: Juan Gaertner/Science Photo Library",
            "Joylessness triggered by stress creates a distinct brain signature, according to research in mice1. The study also reveals one brain pattern that seems to confer resilience to stress \u2014 and another that makes stressed animals less likely to feel pleasure, a core symptom of depression. ",
            "These findings, published today in Nature, offer clues as to how the brain gives rise to anhedonia, a resistance to enjoyment and pleasure. The results also provide a new avenue for treating the condition \u2014 if the findings are validated in humans.",
            "\u201cTheir approach in this study is spot on,\u201d says Conor Liston, a neuroscientist at Weill Cornell Medicine in New York City, who was not involved in the work. The experiments fill \u201ca big gap\u201d, he says. \u201cAnhedonia is something we don\u2019t understand very well.\u201d",
            "h2A distressing symptom",
            "More than 70% of people with severe depression experience anhedonia, which is also common in those with schizophrenia, Parkinson\u2019s disease and other neurological and psychiatric conditions.",
            "The symptom is notoriously difficult to treat, even in those taking medication, Liston says. \u201cAnhedonia is something that patients care about the most, and feel like it\u2019s least addressed by current treatments,\u201d he says.",
            "A journey into the causes and effects of depression",
            "To understand how the brain gives rise to anhedonia, Mazen Kheirbek, a systems neuroscientist at the University of California, San Francisco, and his colleagues studied mice that had been placed under stress by exposure to larger, more aggressive mice.",
            "Typically, mice have a sweet tooth and prefer sugar water over plain water if given the option. But some stressed mice instead preferred plain water \u2014 which Kheirbek and his colleagues interpreted as a rodent version of anhedonia. Other mice subjected to the same stress preferred the sugar water. The authors labelled these animals \u2018resilient\u2019.",
            "The researchers then monitored neurons in the amygdala and hippocampus, two brain regions that are important for processing emotions, in mice that were deciding between sugar water and plain water after earlier exposure to stress.",
            "h2Building a resilient brain",
            "Resilient mice had robust communication between the amygdala and hippocampus, whereas in animals susceptible to anhedonia, communication between the two brain areas was fragmented.",
            "To improve the disjointed communication in susceptible mice, the researchers injected the rodents with compounds that caused neurons in the target areas to fire more frequently. These animals opted for sugar water more often than they had before the injections, and their brain activity was more similar to that of the resilient mice, the authors found. ",
            "doi: https://doi.org/10.1038/d41586-024-03958-2"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-03957-3",
        "title": "DeepMind AI weather forecaster beats world-class system",
        "content": [
            "*A_satellite_photo_of_typhoon_Hagibis_approaching_Japan",
            "Typhoon Hagibis approaches Japan in 2019. The storm was one of the events used to study the accuracy of an AI-based forecasting system.Credit: NASA Worldview, Earth Observing System Data and Information System (EOSDIS)/AP/Alamy",
            "Google DeepMind has developed the first artificial intelligence (AI) model of its kind to predict the weather more accurately than the best system currently in use. The model generates forecasts up to 15 days in advance \u2014 and it does so in minutes, rather than the hours needed by today\u2019s forecasting programs. ",
            "The purely AI system beats the world\u2019s best medium-range operational model, the European Centre for Medium-Range Weather Forecasts\u2019 ensemble model (ENS), at predicting extreme weather such as hurricanes and heatwaves. The breakthrough could help usher in an era of AI weather forecasting that is quicker and more reliable than today\u2019s systems, researchers say. The system, called GenCast, is described today in Nature1.",
            "Superfast Microsoft AI is first to predict air pollution for the whole world",
            "Conventional forecasts, including those from ENS, are based on mathematical models that simulate the laws of physics governing Earth\u2019s atmosphere. They use supercomputers to crunch data from satellites and weather stations \u2014 a process that takes hours and vast amounts of computing power.",
            "GenCast, by contrast, has been trained only on historical weather data, which enables the system to draw out complex relationships between variables such as air pressure, humidity, temperature and wind. This helps it to outperform strictly physics-based systems, says Ilan Price, a research scientist at Google DeepMind in London and an author of the paper. ",
            "\u201cWe\u2019ve really made dramatic progress to catch up and now overtake [physics-based models] with machine learning,\u201d Price says. ",
            "h2AI surge",
            "AI weather forecasting has advanced rapidly, with multiple companies racing to develop new and better models. Among them are Huawei2 in Shenzhen, China, and Nvidia in Santa Clara, California. Earlier this year, Google released NeuralGCM3, a hybrid system that combines physics-based models with AI to produce short- and long-term forecasts on a par with conventional models. ",
            "doi: https://doi.org/10.1038/d41586-024-03957-3"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-01684-3",
        "title": "Virtual lab powered by \u2018AI scientists\u2019 super-charges biomedical research",
        "content": [
            "*Illustration_of_antibodies_red_and_blue_responding_to_SARS-CoV-2_purple",
            "The virtual lab set-up used several LLMs to design antibody fragments that could bind to SARS-CoV-2.Credit: KTSDESIGN/Science Photo Library via Getty",
            "In an effort to automate scientific discovery using artificial intelligence (AI), researchers have created a virtual laboratory that combines several \u2018AI scientists\u2019 \u2014 large language models with defined scientific roles \u2014 that can collaborate to achieve goals set by human researchers.",
            "The system, described in a preprint posted on bioRxiv last month1, was able to design antibody fragments called nanobodies that can bind to the virus that causes COVID-19, proposing nearly 100 of these structures in a fraction of the time it would take an all-human research group. ",
            "Researchers built an \u2018AI Scientist\u2019 \u2014 what can it do?",
            " \u201cThese virtual-lab AI agents have shown to be quite capable at doing a lot of tasks,\u201d says study co-author James Zou, a computational biologist at Stanford University in California. \u201cWe\u2019re quite excited about exploring the potential of the virtual lab across different scientific domains.\u201d",
            "The experiment \u201crepresents a new paradigm of taking AI as collaborators, not just tools\u201d, says Yanjun Gao, who researches the health-care applications of AI at the University of Colorado Anschutz Medical Campus in Aurora. But she adds that human input and oversight are still crucial. \u201cI don\u2019t think at this stage we can fully trust the AI to make decisions.\u201d",
            "h2Interdisciplinary AI",
            "Scientists worldwide have explored the potential of large language models (LLMs) to speed up research \u2014 including creating an \u2018AI scientist\u2019 that can carry out parts of the scientific process, from generating hypotheses and designing experiments to drafting papers. But Zou says that most studies have focused on the application of LLMs for experiments with a narrow scope, rather than exploring their potential in interdisciplinary research. He and his colleagues set up the virtual lab to combine expertise from different fields. ",
            "They began by training two LLMs for their virtual team: the team-leading principal investigator (PI), which has expertise in AI for research, and a \u2018scientific critic\u2019 to catch errors and oversights from other LLMs throughout the process. The authors gave these LLMs a goal \u2014 designing new nanobodies to target the virus SARS-CoV-2 \u2014 and instructed them to develop other LLMs that could achieve it. ",
            "\u2018A landmark moment\u2019: scientists use AI to design antibodies from scratch",
            "The PI then created and trained three further AI scientist agents to support the research efforts. Each of these \u2018scientists\u2019 was trained in a particular discipline \u2014 immunology, computational biology or machine learning. \u201cThese different agents would have different expertise, and they would work together in solving different kinds of scientific problems,\u201d says Zou. ",
            "The AI agents worked independently on tasks allocated by the virtual PI, such as calculating parameters or writing code for a new machine-learning model. They could also make use of other AI research tools, such as the protein-design tools AlphaFold and Rosetta. A human researcher guided the LLMs through regular \u2018team meetings\u2019 to evaluate their progress. ",
            "doi: https://doi.org/10.1038/d41586-024-01684-3"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-03905-1",
        "title": "How close is AI to human-level intelligence?",
        "content": [
            "*Abstract_illustration_showing_a_brain_being_formed_from_data_surrounded_by_representations_of_3D_map",
            " Illustration: Petra P\u00e9terffy",
            "OpenAI\u2019s latest artificial intelligence (AI) system dropped in September with a bold promise. The company behind the chatbot ChatGPT showcased o1 \u2014 its latest suite of large language models (LLMs) \u2014 as having a \u201cnew level of AI capability\u201d. OpenAI, which is based in San Francisco, California, claims that o1 works in a way that is closer to how a person thinks than do previous LLMs.",
            "The release poured fresh fuel on a debate that\u2019s been simmering for decades: just how long will it be until a machine is capable of the whole range of cognitive tasks that human brains can handle, including generalizing from one task to another, abstract reasoning, planning and choosing which aspects of the world to investigate and learn from?",
            "Bigger AI chatbots more inclined to spew nonsense \u2014 and people don\u2019t always realize",
            "Such an \u2018artificial general intelligence\u2019, or AGI, could tackle thorny problems, including climate change, pandemics and cures for cancer, Alzheimer\u2019s and other diseases. But such huge power would also bring uncertainty \u2014 and pose risks to humanity. \u201cBad things could happen because of either the misuse of AI or because we lose control of it,\u201d says Yoshua Bengio, a deep-learning researcher at the University of Montreal, Canada.",
            "The revolution in LLMs over the past few years has prompted speculation that AGI might be tantalizingly close. But given how LLMs are built and trained, they will not be sufficient to get to AGI on their own, some researchers say. \u201cThere are still some pieces missing,\u201d says Bengio.",
            "What\u2019s clear is that questions about AGI are now more relevant than ever. \u201cMost of my life, I thought people talking about AGI are crackpots,\u201d says Subbarao Kambhampati, a computer scientist at Arizona State University in Tempe. \u201cNow, of course, everybody is talking about it. You can\u2019t say everybody\u2019s a crackpot.\u201d",
            "h2Why the AGI debate changed",
            "The phrase artificial general intelligence entered the zeitgeist around 2007 after its mention in an eponymously named book edited by AI researchers Ben Goertzel and Cassio Pennachin. Its precise meaning remains elusive, but it broadly refers to an AI system with human-like reasoning and generalization abilities. Fuzzy definitions aside, for most of the history of AI, it\u2019s been clear that we haven\u2019t yet reached AGI. Take AlphaGo, the AI program created by Google DeepMind to play the board game Go. It beats the world\u2019s best human players at the game \u2014 but its superhuman qualities are narrow, because that\u2019s all it can do.",
            "The new capabilities of LLMs have radically changed the landscape. Like human brains, LLMs have a breadth of abilities that have caused some researchers to seriously consider the idea that some form of AGI might be imminent1, or even already here.",
            "This breadth of capabilities is particularly startling when you consider that researchers only partially understand how LLMs achieve it. An LLM is a neural network, a machine-learning model loosely inspired by the brain; the network consists of artificial neurons, or computing units, arranged in layers, with adjustable parameters that denote the strength of connections between the neurons. During training, the most powerful LLMs \u2014 such as o1, Claude (built by Anthropic in San Francisco) and Google\u2019s Gemini \u2014 rely on a method called next token prediction, in which a model is repeatedly fed samples of text that has been chopped up into chunks known as tokens. These tokens could be entire words or simply a set of characters. The last token in a sequence is hidden or \u2018masked\u2019 and the model is asked to predict it. The training algorithm then compares the prediction with the masked token and adjusts the model\u2019s parameters to enable it to make a better prediction next time.",
            "How AI is reshaping science and society",
            "The process continues \u2014 typically using billions of fragments of language, scientific text and programming code \u2014 until the model can reliably predict the masked tokens. By this stage, the model parameters have captured the statistical structure of the training data, and the knowledge contained therein. The parameters are then fixed and the model uses them to predict new tokens when given fresh queries or \u2018prompts\u2019 that were not necessarily present in its training data, a process known as inference.",
            "The use of a type of neural network architecture known as a transformer has taken LLMs significantly beyond previous achievements. The transformer allows a model to learn that some tokens have a particularly strong influence on others, even if they are widely separated in a sample of text. This permits LLMs to parse language in ways that seem to mimic how humans do it \u2014 for example, differentiating between the two meanings of the word \u2018bank\u2019 in this sentence: \u201cWhen the river\u2019s bank flooded, the water damaged the bank\u2019s ATM, making it impossible to withdraw money.\u201d",
            "This approach has turned out to be highly successful in a wide array of contexts, including generating computer programs to solve problems that are described in natural language, summarizing academic articles and answering mathematics questions.",
            "And other new capabilities have emerged along the way, especially as LLMs have increased in size, raising the possibility that AGI, too, could simply emerge if LLMs get big enough. One example is chain-of-thought (CoT) prompting. This involves showing an LLM an example of how to break down a problem into smaller steps to solve it, or simply asking the LLM to solve a problem step-by-step. CoT prompting can lead LLMs to correctly answer questions that previously flummoxed them. But the process doesn\u2019t work very well with small LLMs.",
            "h2The limits of LLMs",
            "CoT prompting has been integrated into the workings of o1, according to OpenAI, and underlies the model\u2019s prowess. Francois Chollet, who was an AI researcher at Google in Mountain View, California, and left in November to start a new company, thinks that the model incorporates a CoT generator that creates numerous CoT prompts for a user query and a mechanism to select a good prompt from the choices. During training, o1 is taught not only to predict the next token, but also to select the best CoT prompt for a given query. The addition of CoT reasoning explains why, for example, o1-preview \u2014 the advanced version of o1 \u2014 correctly solved 83% of problems in a qualifying exam for the International Mathematical Olympiad, a prestigious mathematics competition for high-school students, according to OpenAI. That compares with a score of just 13% for the company\u2019s previous most powerful LLM, GPT-4o.",
            "In AI, is bigger always better?",
            "But, despite such sophistication, o1 has its limitations and does not constitute AGI, say Kambhampati and Chollet. On tasks that require planning, for example, Kambhampati\u2019s team has shown that although o1 performs admirably on tasks that require up to 16 planning steps, its performance degrades rapidly when the number of steps increases to between 20 and 402. Chollet saw similar limitations when he challenged o1-preview with a test of abstract reasoning and generalization that he designed to measure progress towards AGI. The test takes the form of visual puzzles. Solving them requires looking at examples to deduce an abstract rule and using that to solve new instances of a similar puzzle, something humans do with relative ease.",
            "LLMs, says Chollet, irrespective of their size, are limited in their ability to solve problems that require recombining what they have learnt to tackle new tasks. \u201cLLMs cannot truly adapt to novelty because they have no ability to basically take their knowledge and then do a fairly sophisticated recombination of that knowledge on the fly to adapt to new context.\u201d",
            "h2Can LLMs deliver AGI?",
            "So, will LLMs ever deliver AGI? One point in their favour is that the underlying transformer architecture can process and find statistical patterns in other types of information in addition to text, such as images and audio, provided that there is a way to appropriately tokenize those data. Andrew Wilson, who studies machine learning at New York University in New York City, and his colleagues showed that this might be because the different types of data all share a feature: such data sets have low \u2018Kolmogorov complexity\u2019, defined as the length of the shortest computer program that\u2019s required to create them3. The researchers also showed that transformers are well-suited to learning about patterns in data with low Kolmogorov complexity and that this suitability grows with the size of the model. Transformers have the capacity to model a wide swathe of possibilities, increasing the chance that the training algorithm will discover an appropriate solution to a problem, and this \u2018expressivity\u2019 increases with size. These are, says Wilson, \u201csome of the ingredients that we really need for universal learning\u201d. Although Wilson thinks AGI is currently out of reach, he says that LLMs and other AI systems that use the transformer architecture have some of the key properties of AGI-like behaviour.",
            "Can AI review the scientific literature \u2014 and figure out what it all means?",
            "Yet there are also signs that transformer-based LLMs have limits. For a start, the data used to train the models are running out. Researchers at Epoch AI, an institute in San Francisco that studies trends in AI, estimate4 that the existing stock of publicly available textual data used for training might run out somewhere between 2026 and 2032. There are also signs that the gains being made by LLMs as they get bigger are not as great as they once were, although it\u2019s not clear if this is related to there being less novelty in the data because so many have now been used, or something else. The latter would bode badly for LLMs.",
            "Raia Hadsell, vice-president of research at Google DeepMind in London, raises another problem. The powerful transformer-based LLMs are trained to predict the next token, but this singular focus, she argues, is too limited to deliver AGI. Building models that instead generate solutions all at once or in large chunks could bring us closer to AGI, she says. The algorithms that could help to build such models are already at work in some existing, non-LLM systems, such as OpenAI\u2019s DALL-E, which generates realistic, sometimes trippy, images in response to descriptions in natural language. But they lack LLMs\u2019 broad suite of capabilities.",
            "h2Build me a world model",
            "The intuition for what breakthroughs are needed to progress to AGI comes from neuroscientists. They argue that our intelligence is the result of the brain being able to build a \u2018world model\u2019, a representation of our surroundings. This can be used to imagine different courses of action and predict their consequences, and therefore to plan and reason. It can also be used to generalize skills that have been learnt in one domain to new tasks by simulating different scenarios.",
            "Nature 636, 22-25 (2024)",
            "doi: https://doi.org/10.1038/d41586-024-03905-1"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-03939-5",
        "title": "Huge randomized trial of AI boosts discovery \u2014 at least for good scientists",
        "content": [
            "*A_researcher_pictured_using_a_FTIR_spectrophotometer",
            "Scientists at an unnamed corporate laboratory were randomly assigned a machine-learning tool.Credit: Eugenio Marongiu/Getty",
            "Artificial intelligence (AI) is becoming ubiquitous in applied research, but can it actually invent useful materials faster than humans can? It is still too early to tell, but a massive study suggests that it might.",
            "doi: https://doi.org/10.1038/d41586-024-03939-5"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-04017-6",
        "title": "A science mega-programme is taking shape in the EU: what it means for researchers",
        "content": [
            "*Commissioners-Designate_Ekaterina_Zakharieva_speaks_during_a_European_Parliament_hearing_in_Belgium",
            "EU research commissioner Ekaterina Zakharieva held positions in the Bulgarian government.Credit: Thierry Monasse/Getty",
            "The European Union has a new research head, tasked with reshaping the world\u2019s biggest collaborative research programme to help stop the bloc\u2019s economic and technological downward slide.",
            "doi: https://doi.org/10.1038/d41586-024-04017-6"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-03982-2",
        "title": "Wuhan lab samples hold no close relatives to virus behind COVID",
        "content": [
            "*Shi_Zhengli_wearing_protective_clothing_in_a_biosafety_lab",
            "Chinese virologist Shi Zhengli has presented evidence that her lab has not worked with close relatives of SARS-CoV-2.Credit: Johannes Eisele/AFP via Getty",
            "After years of rumours that the virus that causes COVID-19 escaped from a laboratory in China, the virologist at the centre of the claims has presented data on dozens of new coronaviruses collected from bats in southern China. At a conference in Japan this week, Shi Zhengli, a specialist on bat coronaviruses, reported that none of the viruses stored in her freezers are the most recent ancestors of the virus SARS-CoV-2.",
            "Shi was leading coronavirus research at the Wuhan Institute of Virology (WIV), a high-level biosafety laboratory, when the first cases of COVID-19 were reported in that city. Soon afterwards, theories emerged that the virus had leaked \u2014 either by accident or deliberately \u2014 from the WIV.",
            "Shi has consistently said that SARS-CoV-2 was never seen or studied in her lab. But some commentators have continued to ask whether one of the many bat coronaviruses her team collected in southern China over decades was closely related to it. Shi promised to sequence the genomes of the coronaviruses and release the data.",
            "The latest analysis, which has not been peer reviewed, includes data from the whole genomes of 56 new betacoronaviruses, the broad group to which SARS-CoV-2 belongs, as well as some partial sequences. All the viruses were collected between 2004 and 2021.",
            "\u201cWe didn\u2019t find any new sequences which are more closely related to SARS-CoV-1 and SARS-CoV-2,\u201d said Shi, in a pre-recorded presentation at the conference, Preparing for the Next Pandemic: Evolution, Pathogenesis and Virology of Coronaviruses, in Awaji, Japan, on 4 December. Earlier this year, Shi moved from the WIV to the Guangzhou Laboratory, a newly established national research institute for infectious diseases.",
            "The results support her assertion that the WIV lab did not have any bat-derived sequences from viruses that were more closely related to SARS-CoV-2 than were any already described in scientific papers, says Jonathan Pekar, an evolutionary biologist at the University of Edinburgh, UK. \u201cThis just validates what she was saying: that she did not have anything extremely closely related, as we\u2019ve seen in the years since,\u201d he says.",
            "doi: https://doi.org/10.1038/d41586-024-03982-2"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-03968-0",
        "title": "Sick animals suggest COVID pandemic started in Wuhan market",
        "content": [
            "*Close_up_of_a_raccoon_dog_in_a_cage_in_rural_China",
            "Raccoon dogs are among the animals susceptible to SARS-CoV-2 that were present at the Chinese market where the virus is thought to have jumped to humans.Credit: YongXin Zhang/Alamy",
            "The quest to understand where the COVID-19 pandemic started has revealed fresh clues. Researchers have re-analysed data collected from a market in Wuhan, China, during the early days of the pandemic and found that animals there were infected with a virus \u2013 although they could not confirm what exactly caused the infection.",
            "\u201cThe conclusion is convincing that there was infection in the animals,\u201d says Spyros Lytras, an evolutionary virologist at the University of Tokyo. The results, which have not been peer reviewed, were presented at a conference, Preparing for the Next Pandemic: Evolution, Pathogenesis and Virology of Coronaviruses, in Awaji, Japan, on 3 December.",
            "Many of the first COVID-19 cases to be identified were linked to Wuhan\u2019s Huanan Seafood Wholesale Market. Some studies have reasoned that people brought the virus to the market, where they passed it on to others, whereas other studies have suggested that the market was the site of the first spillover events, in which animals with the virus first infected people1. Although these earlier studies established the presence of animals susceptible to the virus that causes COVID-19, and the virus itself, at the market, they were not able to confirm that the animals were infected with the virus2.",
            "\u201cThe missing link in the whole zoonotic story has been the animal,\u201d says Edward Holmes, a virologist at the University of Sydney in Australia. \u201cIf you can show there are infected animals at the market, then the story is complete,\u201d he says, referring specifically to animals infected with a progenitor of SARS-CoV-2.",
            "The latest analysis suggests that infected animals were at the market at the same time that early cases of COVID-19 emerged there. \u201cThis is one more piece of indirect evidence that suggests a connection of the origin of the SARS-CoV-2 pandemic with the Huanan market,\u201d says Christian Drosten, a virologist at the Charit\u00e9 University Hospital in Berlin.",
            "Most researchers agree that SARS-CoV-2 originated in animals. However, because a progenitor of the virus has not been found in an animal, some have continued to argue that the virus could have escaped \u2014 either by accident or through deliberate release \u2014 from the Wuhan Institute of Virology. A detailed report released by the Republican-majority select committee of the US House of Representatives earlier this week concluded that the pandemic \u201cmost likely emerged from a laboratory in Wuhan\u201d.",
            "h2Infected animals",
            "Shortly after the Huanan market was shut down on 1 January 2020, a group of researchers from the Chinese Center for Disease Control and Prevention in Beijing visited the market and swabbed stalls, walls, bins, sewage wells and animal products stored in freezers. They sequenced DNA and RNA from those swabs and deposited the resulting data into a genomic database.",
            "Angela Rasmussen, a virologist at the University of Saskatchewan in Saskatoon, Canada, wanted to look more closely at the data for potential animal intermediates. She studied the genomic data of animals found at the market that are susceptible to SARS-CoV-2. These included American mink (Neogale vison), ermine (Mustela erminea), masked palm civets (Paguma larvata), raccoon dogs (Nyctereutes procyonoides), red foxes (Vulpes vulpes) and greater hog badgers (Arctonyx collaris).",
            "doi: https://doi.org/10.1038/d41586-024-03968-0"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-03936-8",
        "title": "What is ageing? Even the field\u2019s researchers can\u2019t agree",
        "content": [
            "*Young_and_old_hands_on_a_black_background_show_the_contrast_of_ageing",
            "When ageing begins is one of many questions researchers cannot agree on.Credit: mrPliskin/iStock via Getty",
            "Researchers studying ageing disagree on just about everything \u2014 including what ageing is, whether it is a disease and when it starts \u2014 according to a survey of about 100 scientists working in the field.",
            "A key goal of ageing research is to help people live longer, healthier lives. But the exact causes of ageing, as well as effective approaches to slow or reverse it, remain elusive. For the field to tackle these challenges, researchers need to speak a common language, says Alan Cohen, who studies ageing at Columbia University in New York City. \u201cThere doesn\u2019t have to be perfect consensus, but we need to sort things out quite a bit,\u201d he says.",
            "Vadim Gladyshev, another researcher in the field who is based at Harvard Medical School in Boston, Massachusetts, and his colleagues agree. They decided to survey participants at an international conference on ageing in Newry, Maine, in 2022, to better understand the views of those researching the topic. Respondents included early-career researchers, established scientists and industry professionals. The results are described in PNAS Nexus today1.",
            "Most researchers are clear in their own minds about what ageing is \u2014 but their perspectives don\u2019t align with those of others, says Gladyshev. \u201cPeople joke in the field that there are more theories than people.\u201d Despite this, Gladyshev says he was surprised by the scale of the problem.",
            "The latest results reflect those of a similar survey of 37 researchers conducted in 2019 by Cohen and his colleagues2. Now \u201cit\u2019s unquestionably clear that there\u2019s a huge disagreement\u201d, says Cohen.",
            "h2What is ageing?",
            "When asked to describe ageing, one-third of respondents considered it to be a loss of function over time, from declines at the cellular level to a decrease in overall health and fitness. Others saw ageing as a gradual accumulation of deleterious changes. Not all respondents associated ageing with negative connotations, with some seeing it as a change in state \u2014 reversible or otherwise \u2014 or a continuation of development. And others approached the subject from a demographic standpoint, describing ageing simply as an increased chance of dying.",
            "A question about the causes of ageing also elicited a broad range of responses, from the build-up of damage to evolutionary constraints, and from changes to the regulatory system to a deterioration in repair mechanisms. A few admitted they did not know what underlies ageing. ",
            "Researchers also disagree on whether ageing is a disease. More than one-third of respondents said it is, another 38% said it isn\u2019t and the remaining 28% were neutral. Cohen doesn\u2019t favour describing ageing as a disease because it implies that it is something that needs to be eliminated, although many researchers in the field are, to some extent, working towards this goal.",
            "For Gladyshev, the answer to the question is more complicated. \u201cAgeing is not a disease, but it is also not not a disease,\u201d he says. He sees many diseases as essentially accelerated ageing taking place in specific organs or in the body as a whole.",
            "h2When does ageing start?",
            "doi: https://doi.org/10.1038/d41586-024-03936-8"
        ]
    }
]