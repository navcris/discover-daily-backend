[
    {
        "URL": "https://www.nature.com/articles/d41586-024-01684-3",
        "title": "Virtual lab powered by \u2018AI scientists\u2019 super-charges biomedical research",
        "content": [
            "*Illustration_of_antibodies_red_and_blue_responding_to_SARS-CoV-2_purple",
            "The virtual lab set-up used several LLMs to design antibody fragments that could bind to SARS-CoV-2.Credit: KTSDESIGN/Science Photo Library via Getty",
            "In an effort to automate scientific discovery using artificial intelligence (AI), researchers have created a virtual laboratory that combines several \u2018AI scientists\u2019 \u2014 large language models with defined scientific roles \u2014 that can collaborate to achieve goals set by human researchers.",
            "The system, described in a preprint posted on bioRxiv last month1, was able to design antibody fragments called nanobodies that can bind to the virus that causes COVID-19, proposing nearly 100 of these structures in a fraction of the time it would take an all-human research group. ",
            "Researchers built an \u2018AI Scientist\u2019 \u2014 what can it do?",
            " \u201cThese virtual-lab AI agents have shown to be quite capable at doing a lot of tasks,\u201d says study co-author James Zou, a computational biologist at Stanford University in California. \u201cWe\u2019re quite excited about exploring the potential of the virtual lab across different scientific domains.\u201d",
            "The experiment \u201crepresents a new paradigm of taking AI as collaborators, not just tools\u201d, says Yanjun Gao, who researches the health-care applications of AI at the University of Colorado Anschutz Medical Campus in Aurora. But she adds that human input and oversight are still crucial. \u201cI don\u2019t think at this stage we can fully trust the AI to make decisions.\u201d",
            "h2Interdisciplinary AI",
            "Scientists worldwide have explored the potential of large language models (LLMs) to speed up research \u2014 including creating an \u2018AI scientist\u2019 that can carry out parts of the scientific process, from generating hypotheses and designing experiments to drafting papers. But Zou says that most studies have focused on the application of LLMs for experiments with a narrow scope, rather than exploring their potential in interdisciplinary research. He and his colleagues set up the virtual lab to combine expertise from different fields. ",
            "doi: https://doi.org/10.1038/d41586-024-01684-3"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-03957-3",
        "title": "DeepMind AI weather forecaster beats world-class system",
        "content": [
            "*A_satellite_photo_of_typhoon_Hagibis_approaching_Japan",
            "Typhoon Hagibis approaches Japan in 2019. The storm was one of the events used to study the accuracy of an AI-based forecasting system.Credit: NASA Worldview, Earth Observing System Data and Information System (EOSDIS)/AP/Alamy",
            "Google DeepMind has developed the first artificial intelligence (AI) model of its kind to predict the weather more accurately than the best system currently in use. The model generates forecasts up to 15 days in advance \u2015 and it does so in minutes, rather than the hours needed by today\u2019s forecasting programs. ",
            "doi: https://doi.org/10.1038/d41586-024-03957-3"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-03958-2",
        "title": "Stress can dull our capacity for joy: mouse brain patterns hint at why",
        "content": [
            "*Illustration_of_nerve_cells_from_the_cerebral_cortex_of_the_brain,_shown_in_blue",
            "Communication between neurons (illustration) in two separate brain regions is patchy in mice that are susceptible to severe stress. Credit: Juan Gaertner/Science Photo Library",
            "Joylessness triggered by stress creates a distinct brain signature, according to research in mice1. The study also reveals one brain pattern that seems to confer resilience to stress \u2015 and another that makes stressed animals less likely to feel pleasure, a core symptom of depression. ",
            "These findings, published today in Nature, offer clues as to how the brain gives rise to anhedonia, a resistance to enjoyment and pleasure. The results also provide a new avenue for treating the condition \u2015 if the findings are validated in humans.",
            "\u201cTheir approach in this study is spot on,\u201d says Conor Liston, a neuroscientist at Weill Cornell Medicine in New York City, who was not involved in the work. The experiments fill \u201ca big gap\u201d, he says. \u201cAnhedonia is something we don\u2019t understand very well.\u201d",
            "h2A distressing symptom",
            "More than 70% of people with severe depression experience anhedonia, which is also common in those with schizophrenia, Parkinson\u2019s disease and other neurological and psychiatric conditions.",
            "The symptom is notoriously difficult to treat, even in those taking medication, Liston says. \u201cAnhedonia is something that patients care about the most, and feel like it\u2019s least addressed by current treatments,\u201d he says.",
            "A journey into the causes and effects of depression",
            "To understand how the brain gives rise to anhedonia, Mazen Kheirbek, a systems neuroscientist at the University of California, San Francisco, and his colleagues studied mice that had been placed under stress by exposure to larger, more aggressive mice.",
            "Typically, mice have a sweet tooth and prefer sugar water over plain water if given the option. But some stressed mice instead preferred plain water \u2015 which Kheirbek and his colleagues interpreted as a rodent version of anhedonia. Other mice subjected to the same stress preferred the sugar water. The authors labelled these animals \u2018resilient\u2019.",
            "doi: https://doi.org/10.1038/d41586-024-03958-2"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-03905-1",
        "title": "How close is AI to human-level intelligence?",
        "content": [
            "*Abstract_illustration_showing_a_brain_being_formed_from_data_surrounded_by_representations_of_3D_map",
            " Illustration: Petra P\u00e9terffy",
            "OpenAI\u2019s latest artificial intelligence (AI) system dropped in September with a bold promise. The company behind the chatbot ChatGPT showcased o1 \u2014 its latest suite of large language models (LLMs) \u2014 as having a \u201cnew level of AI capability\u201d. OpenAI, which is based in San Francisco, California, claims that o1 works in a way that is closer to how a person thinks than do previous LLMs.",
            "The release poured fresh fuel on a debate that\u2019s been simmering for decades: just how long will it be until a machine is capable of the whole range of cognitive tasks that human brains can handle, including generalizing from one task to another, abstract reasoning, planning and choosing which aspects of the world to investigate and learn from?",
            "Bigger AI chatbots more inclined to spew nonsense \u2014 and people don\u2019t always realize",
            "Such an \u2018artificial general intelligence\u2019, or AGI, could tackle thorny problems, including climate change, pandemics and cures for cancer, Alzheimer\u2019s and other diseases. But such huge power would also bring uncertainty \u2014 and pose risks to humanity. \u201cBad things could happen because of either the misuse of AI or because we lose control of it,\u201d says Yoshua Bengio, a deep-learning researcher at the University of Montreal, Canada.",
            "The revolution in LLMs over the past few years has prompted speculation that AGI might be tantalizingly close. But given how LLMs are built and trained, they will not be sufficient to get to AGI on their own, some researchers say. \u201cThere are still some pieces missing,\u201d says Bengio.",
            "What\u2019s clear is that questions about AGI are now more relevant than ever. \u201cMost of my life, I thought people talking about AGI are crackpots,\u201d says Subbarao Kambhampati, a computer scientist at Arizona State University in Tempe. \u201cNow, of course, everybody is talking about it. You can\u2019t say everybody\u2019s a crackpot.\u201d",
            "h2Why the AGI debate changed",
            "The phrase artificial general intelligence entered the zeitgeist around 2007 after its mention in an eponymously named book edited by AI researchers Ben Goertzel and Cassio Pennachin. Its precise meaning remains elusive, but it broadly refers to an AI system with human-like reasoning and generalization abilities. Fuzzy definitions aside, for most of the history of AI, it\u2019s been clear that we haven\u2019t yet reached AGI. Take AlphaGo, the AI program created by Google DeepMind to play the board game Go. It beats the world\u2019s best human players at the game \u2014 but its superhuman qualities are narrow, because that\u2019s all it can do.",
            "The new capabilities of LLMs have radically changed the landscape. Like human brains, LLMs have a breadth of abilities that have caused some researchers to seriously consider the idea that some form of AGI might be imminent1, or even already here.",
            "This breadth of capabilities is particularly startling when you consider that researchers only partially understand how LLMs achieve it. An LLM is a neural network, a machine-learning model loosely inspired by the brain; the network consists of artificial neurons, or computing units, arranged in layers, with adjustable parameters that denote the strength of connections between the neurons. During training, the most powerful LLMs \u2014 such as o1, Claude (built by Anthropic in San Francisco) and Google\u2019s Gemini \u2014 rely on a method called next token prediction, in which a model is repeatedly fed samples of text that has been chopped up into chunks known as tokens. These tokens could be entire words or simply a set of characters. The last token in a sequence is hidden or \u2018masked\u2019 and the model is asked to predict it. The training algorithm then compares the prediction with the masked token and adjusts the model\u2019s parameters to enable it to make a better prediction next time.",
            "How AI is reshaping science and society",
            "The process continues \u2014 typically using billions of fragments of language, scientific text and programming code \u2014 until the model can reliably predict the masked tokens. By this stage, the model parameters have captured the statistical structure of the training data, and the knowledge contained therein. The parameters are then fixed and the model uses them to predict new tokens when given fresh queries or \u2018prompts\u2019 that were not necessarily present in its training data, a process known as inference.",
            "The use of a type of neural network architecture known as a transformer has taken LLMs significantly beyond previous achievements. The transformer allows a model to learn that some tokens have a particularly strong influence on others, even if they are widely separated in a sample of text. This permits LLMs to parse language in ways that seem to mimic how humans do it \u2014 for example, differentiating between the two meanings of the word \u2018bank\u2019 in this sentence: \u201cWhen the river\u2019s bank flooded, the water damaged the bank\u2019s ATM, making it impossible to withdraw money.\u201d",
            "This approach has turned out to be highly successful in a wide array of contexts, including generating computer programs to solve problems that are described in natural language, summarizing academic articles and answering mathematics questions.",
            "And other new capabilities have emerged along the way, especially as LLMs have increased in size, raising the possibility that AGI, too, could simply emerge if LLMs get big enough. One example is chain-of-thought (CoT) prompting. This involves showing an LLM an example of how to break down a problem into smaller steps to solve it, or simply asking the LLM to solve a problem step-by-step. CoT prompting can lead LLMs to correctly answer questions that previously flummoxed them. But the process doesn\u2019t work very well with small LLMs.",
            "h2The limits of LLMs",
            "CoT prompting has been integrated into the workings of o1, according to OpenAI, and underlies the model\u2019s prowess. Francois Chollet, who was an AI researcher at Google in Mountain View, California, and left in November to start a new company, thinks that the model incorporates a CoT generator that creates numerous CoT prompts for a user query and a mechanism to select a good prompt from the choices. During training, o1 is taught not only to predict the next token, but also to select the best CoT prompt for a given query. The addition of CoT reasoning explains why, for example, o1-preview \u2014 the advanced version of o1 \u2014 correctly solved 83% of problems in a qualifying exam for the International Mathematical Olympiad, a prestigious mathematics competition for high-school students, according to OpenAI. That compares with a score of just 13% for the company\u2019s previous most powerful LLM, GPT-4o.",
            "In AI, is bigger always better?",
            "But, despite such sophistication, o1 has its limitations and does not constitute AGI, say Kambhampati and Chollet. On tasks that require planning, for example, Kambhampati\u2019s team has shown that although o1 performs admirably on tasks that require up to 16 planning steps, its performance degrades rapidly when the number of steps increases to between 20 and 402. Chollet saw similar limitations when he challenged o1-preview with a test of abstract reasoning and generalization that he designed to measure progress towards AGI. The test takes the form of visual puzzles. Solving them requires looking at examples to deduce an abstract rule and using that to solve new instances of a similar puzzle, something humans do with relative ease.",
            "LLMs, says Chollet, irrespective of their size, are limited in their ability to solve problems that require recombining what they have learnt to tackle new tasks. \u201cLLMs cannot truly adapt to novelty because they have no ability to basically take their knowledge and then do a fairly sophisticated recombination of that knowledge on the fly to adapt to new context.\u201d",
            "h2Can LLMs deliver AGI?",
            "So, will LLMs ever deliver AGI? One point in their favour is that the underlying transformer architecture can process and find statistical patterns in other types of information in addition to text, such as images and audio, provided that there is a way to appropriately tokenize those data. Andrew Wilson, who studies machine learning at New York University in New York City, and his colleagues showed that this might be because the different types of data all share a feature: such data sets have low \u2018Kolmogorov complexity\u2019, defined as the length of the shortest computer program that\u2019s required to create them3. The researchers also showed that transformers are well-suited to learning about patterns in data with low Kolmogorov complexity and that this suitability grows with the size of the model. Transformers have the capacity to model a wide swathe of possibilities, increasing the chance that the training algorithm will discover an appropriate solution to a problem, and this \u2018expressivity\u2019 increases with size. These are, says Wilson, \u201csome of the ingredients that we really need for universal learning\u201d. Although Wilson thinks AGI is currently out of reach, he says that LLMs and other AI systems that use the transformer architecture have some of the key properties of AGI-like behaviour.",
            "Can AI review the scientific literature \u2014 and figure out what it all means?",
            "Yet there are also signs that transformer-based LLMs have limits. For a start, the data used to train the models are running out. Researchers at Epoch AI, an institute in San Francisco that studies trends in AI, estimate4 that the existing stock of publicly available textual data used for training might run out somewhere between 2026 and 2032. There are also signs that the gains being made by LLMs as they get bigger are not as great as they once were, although it\u2019s not clear if this is related to there being less novelty in the data because so many have now been used, or something else. The latter would bode badly for LLMs.",
            "Raia Hadsell, vice-president of research at Google DeepMind in London, raises another problem. The powerful transformer-based LLMs are trained to predict the next token, but this singular focus, she argues, is too limited to deliver AGI. Building models that instead generate solutions all at once or in large chunks could bring us closer to AGI, she says. The algorithms that could help to build such models are already at work in some existing, non-LLM systems, such as OpenAI\u2019s DALL-E, which generates realistic, sometimes trippy, images in response to descriptions in natural language. But they lack LLMs\u2019 broad suite of capabilities.",
            "h2Build me a world model",
            "The intuition for what breakthroughs are needed to progress to AGI comes from neuroscientists. They argue that our intelligence is the result of the brain being able to build a \u2018world model\u2019, a representation of our surroundings. This can be used to imagine different courses of action and predict their consequences, and therefore to plan and reason. It can also be used to generalize skills that have been learnt in one domain to new tasks by simulating different scenarios.",
            "Several reports have claimed evidence for the emergence of rudimentary world models inside LLMs. In one study5, researchers Wes Gurnee and Max Tegmark at the Massachusetts Institute of Technology in Cambridge claimed that a widely used open-source family of LLMs developed internal representations of the world, the United States and New York City when trained on data sets containing information about these places, although other researchers noted on X (formerly Twitter) that there was no evidence that the LLMs were using the world model for simulations or to learn causal relationships. In another study6, Kenneth Li, a computer scientist at Harvard University in Cambridge and his colleagues reported evidence that a small LLM trained on transcripts of moves made by players of the board game Othello learnt to internally represent the state of the board and used this to correctly predict the next legal move.",
            "Other results, however, show how world models learnt by today\u2019s AI systems can be unreliable. In one such study7, computer scientist Keyon Vafa at Harvard University, and his colleagues used a gigantic data set of the turns taken during taxi rides in New York City to train a transformer-based model to predict the next turn in a sequence, which it did with almost 100% accuracy.",
            "By examining the turns the model generated, the researchers were able to show that it had constructed an internal map to arrive at its answers. But the map bore little resemblance to Manhattan (see \u2018The impossible streets of AI\u2019), \u201ccontaining streets with impossible physical orientations and flyovers above other streets\u201d, the authors write. \u201cAlthough the model does do well in some navigation tasks, it\u2019s doing well with an incoherent map,\u201d says Vafa. And when the researchers tweaked the test data to include unforeseen detours that were not present in the training data, it failed to predict the next turn, suggesting that it was unable to adapt to new situations.",
            "*The_impossible_streets_of_AI_The_results_of_an_AI_system_that_was_trained_to_predict_routes_taken_by",
            "Source: Ref. 7",
            "h2The importance of feedback",
            "One important feature that today\u2019s LLMs lack is internal feedback, says Dileep George, a member of the AGI research team at Google DeepMind in Mountain View, California. The human brain is full of feedback connections that allow information to flow bidirectionally between layers of neurons. This allows information to flow from the sensory system to higher layers of the brain to create world models that reflect our environment. It also means that information from the world models can ripple back down and guide the acquisition of further sensory information. Such bidirectional processes lead, for example, to perceptions, wherein the brain uses world models to deduce the probable causes of sensory inputs. They also enable planning, with world models used to simulate different courses of action.",
            "But current LLMs are able to use feedback only in a tacked-on way. In the case of o1, the internal CoT prompting that seems to be at work \u2014 in which prompts are generated to help answer a query and fed back to the LLM before it produces its final answer \u2014 is a form of feedback connectivity. But, as seen with Chollet\u2019s tests of o1, this doesn\u2019t ensure bullet-proof abstract reasoning.",
            "Why scientists trust AI too much \u2014 and what to do about it",
            "Researchers, including Kambhampati, have also experimented with adding external modules, called verifiers, onto LLMs. These check answers that are generated by an LLM in a specific context, such as for creating viable travel plans, and ask the LLM to rerun the query if the answer is not up to scratch8. Kambhampati\u2019s team showed that LLMs aided by external verifiers were able to create travel plans significantly better than were vanilla LLMs. The problem is that researchers have to design bespoke verifiers for each task. \u201cThere is no universal verifier,\u201d says Kambhampati. By contrast, an AGI system that used this approach would probably need to build its own verifiers to suit situations as they arise, in much the same way that humans can use abstract rules to ensure they are reasoning correctly, even for new tasks.",
            "Efforts to use such ideas to help produce new AI systems are in their infancy. Bengio, for example, is exploring how to create AI systems with different architectures to today\u2019s transformer-based LLMs. One of these, which uses what he calls generative flow networks, would allow a single AI system to learn how to simultaneously build world models and the modules needed to use them for reasoning and planning.",
            "Another big hurdle encountered by LLMs is that they are data guzzlers. Karl Friston, a theoretical neuroscientist at University College London, suggests that future systems could be made more efficient by giving them the ability to decide just how much data they need to sample from the environment to construct world models and make reasoned predictions, rather than simply ingesting all the data they are fed. This, says Friston, would represent a form of agency or autonomy, which might be needed for AGI. \u201cYou don\u2019t see that kind of authentic agency, in say, large language models, or generative AI,\u201d he says. \u201cIf you\u2019ve got any kind of intelligent artefact that can select at some level, I think you\u2019re making an important move towards AGI,\u201d he adds.",
            "AI systems with the ability to build effective world models and integrated feedback loops might also rely less on external data because they could generate their own by running internal simulations, positing counterfactuals and using these to understand, reason and plan. Indeed, in 2018, researchers David Ha, then at Google Brain in Tokyo, and J\u00fcrgen Schmidhuber at the Dalle Molle Institute for Artificial Intelligence Studies in Lugano-Viganelllo, Switzerland, reported9 building a neural network that could efficiently build a world model of an artificial environment, and then use it to train the AI to race virtual cars.",
            "Do AI models produce more original ideas than researchers?",
            "If you think that AI systems with this level of autonomy sound scary, you are not alone. As well as researching how to build AGI, Bengio is an advocate of incorporating safety into the design and regulation of AI systems. He argues that research must focus on training models that can guarantee the safety of their own behaviour \u2014 for instance, by having mechanisms that calculate the probability that the model is violating some specified safety constraint and reject actions if the probability is too high. Also, governments need to ensure safe use. \u201cWe need a democratic process that makes sure individuals, corporations, even the military, use AI and develop AI in ways that are going to be safe for the public,\u201d he says.",
            "So will it ever be possible to achieve AGI? Computer scientists say there is no reason to think otherwise. \u201cThere are no theoretical impediments,\u201d says George. Melanie Mitchell, a computer scientist at the Santa Fe Institute in New Mexico, agrees. \u201cHumans and some other animals are a proof of principle that you can get there,\u201d she says. \u201cI don\u2019t think there\u2019s anything particularly special about biological systems versus systems made of other materials that would, in principle, prevent non-biological systems from becoming intelligent.\u201d",
            "But, even if it is possible there is little consensus about how close its arrival might be: estimates range from just a few years from now to at least ten years away. If an AGI system is created, George says, we\u2019ll know it when we see it. Chollet suspects it will creep up on us. \u201cWhen AGI arrives, it\u2019s not going to be as noticeable or as groundbreaking as you might think,\u201d he says. \u201cIt will take time for AGI to realize its full potential. It will be invented first. Then, you will need to scale it up and apply it before it starts really changing the world.\u201d",
            "Nature 636, 22-25 (2024)",
            "doi: https://doi.org/10.1038/d41586-024-03905-1"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-03939-5",
        "title": "A firm randomly assigned its scientists AI: here\u2019s what happened",
        "content": [
            "*A_researcher_pictured_using_a_FTIR_spectrophotometer",
            "Scientists at an unnamed corporate laboratory were randomly assigned a machine-learning tool.Credit: Eugenio Marongiu/Getty",
            "Artificial intelligence (AI) is becoming ubiquitous in applied research, but can it actually invent useful materials faster than humans can? It is still too early to tell, but a massive study suggests that it might.",
            "How close is AI to human-level intelligence?",
            "Aidan Toner-Rodgers, an economist at the Massachusetts Institute of Technology (MIT) in Cambridge, followed the deployment of a machine-learning tool at an unnamed corporate laboratory employing more than 1,000 researchers. Teams that were randomly assigned to use the tool discovered 44% more new materials and filed 39% more patent applications than did the ones that stuck to their standard workflow, he found. Toner-Rodgers posted the results online last month, and has submitted them to a peer-reviewed journal.",
            "\u201cIt is a very interesting paper,\u201d says Robert Palgrave, a solid-state chemist at University College London, adding that the limited disclosure of the trial\u2019s details makes the results of the AI deployment hard to evaluate. \u201cIt maybe doesn\u2019t surprise me that AI can come up with a lot of suggestions,\u201d Palgrave says. \u201cWhat we\u2019re kind of missing is whether those suggestions were good suggestions or not.\u201d",
            "h2Materials maker",
            "Toner-Rodgers had access to internal data from the lab and interviewed the researchers under the condition that he would not disclose the name of the company or the specific products it designed. He writes that it is a US firm that develops new inorganic materials \u2014 including molecular compounds, crystal structures, glasses and metal alloys \u2014 for use in \u201chealthcare, optics, and industrial manufacturing\u201d.",
            "Researchers built an \u2018AI Scientist\u2019 \u2014 what can it do?",
            "Starting in 2022, the company systematically adopted an AI tool that it had customized to fit its needs. According to Toner-Rodgers, the tool combines graph neural networks \u2014 a popular approach in materials discovery that has been used by DeepMind, Google\u2019s London-based AI firm, among others \u2014 with reinforcement learning. The neural network was pre-trained using data from vast existing databases, including crystal structures and their properties from the Materials Project and molecular structures from the Alexandria Materials Database.",
            "Researchers input requirements for a material\u2019s desired properties into the neural network, and the system suggests structures for new materials that could have those properties. The teams then weed out potential duds \u2014 such as formulas that would not lead to a stable compound \u2014 using their own specialist knowledge and computer simulations. They then attempt to synthesize the candidate structures and, if successful, test them in experiments and even in prototypes of finished products. The results are fed back into the neural network \u2014 the \u2018reinforcement\u2019 stage that helps it to improve its predictive abilities.",
            "h2Mixed results",
            "doi: https://doi.org/10.1038/d41586-024-03939-5"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-03968-0",
        "title": "Sick animals suggest COVID pandemic started in Wuhan market",
        "content": [
            "*Close_up_of_a_raccoon_dog_in_a_cage_in_rural_China",
            "Raccoon dogs are among the animals susceptible to SARS-CoV-2 that were present at the Chinese market where the virus is thought to have jumped to humans.Credit: YongXin Zhang/Alamy",
            "The quest to understand where the COVID-19 pandemic started has revealed fresh clues. Researchers have re-analysed data collected from a market in Wuhan, China, during the early days of the pandemic and found that animals there were infected with a virus \u2013 although they could not confirm what exactly caused the infection.",
            "doi: https://doi.org/10.1038/d41586-024-03968-0"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-03936-8",
        "title": "What is ageing? Even the field\u2019s researchers can\u2019t agree",
        "content": [
            "*Young_and_old_hands_on_a_black_background_show_the_contrast_of_ageing",
            "When ageing begins is one of many questions researchers cannot agree on.Credit: mrPliskin/iStock via Getty",
            "Researchers studying ageing disagree on just about everything \u2014 including what ageing is, whether it is a disease and when it starts \u2014 according to a survey of about 100 scientists working in the field.",
            "A key goal of ageing research is to help people live longer, healthier lives. But the exact causes of ageing, as well as effective approaches to slow or reverse it, remain elusive. For the field to tackle these challenges, researchers need to speak a common language, says Alan Cohen, who studies ageing at Columbia University in New York City. \u201cThere doesn\u2019t have to be perfect consensus, but we need to sort things out quite a bit,\u201d he says.",
            "Vadim Gladyshev, another researcher in the field who is based at Harvard Medical School in Boston, Massachusetts, and his colleagues agree. They decided to survey participants at an international conference on ageing in Newry, Maine, in 2022, to better understand the views of those researching the topic. Respondents included early-career researchers, established scientists and industry professionals. The results are described in PNAS Nexus today1.",
            "Most researchers are clear in their own minds about what ageing is \u2014 but their perspectives don\u2019t align with those of others, says Gladyshev. \u201cPeople joke in the field that there are more theories than people.\u201d Despite this, Gladyshev says he was surprised by the scale of the problem.",
            "The latest results reflect those of a similar survey of 37 researchers conducted in 2019 by Cohen and his colleagues2. Now \u201cit\u2019s unquestionably clear that there\u2019s a huge disagreement\u201d, says Cohen.",
            "h2What is ageing?",
            "When asked to describe ageing, one-third of respondents considered it to be a loss of function over time, from declines at the cellular level to a decrease in overall health and fitness. Others saw ageing as a gradual accumulation of deleterious changes. Not all respondents associated ageing with negative connotations, with some seeing it as a change in state \u2014 reversible or otherwise \u2014 or a continuation of development. And others approached the subject from a demographic standpoint, describing ageing simply as an increased chance of dying.",
            "A question about the causes of ageing also elicited a broad range of responses, from the build-up of damage to evolutionary constraints, and from changes to the regulatory system to a deterioration in repair mechanisms. A few admitted they did not know what underlies ageing. ",
            "Researchers also disagree on whether ageing is a disease. More than one-third of respondents said it is, another 38% said it isn\u2019t and the remaining 28% were neutral. Cohen doesn\u2019t favour describing ageing as a disease because it implies that it is something that needs to be eliminated, although many researchers in the field are, to some extent, working towards this goal.",
            "For Gladyshev, the answer to the question is more complicated. \u201cAgeing is not a disease, but it is also not not a disease,\u201d he says. He sees many diseases as essentially accelerated ageing taking place in specific organs or in the body as a whole.",
            "h2When does ageing start?",
            "Respondents generally said that ageing starts early in life, but they couldn\u2019t agree on how early.",
            "Some said the process begins before conception, when eggs and sperm are being produced. According to this theory , if your parents are older when you are conceived, you are already more advanced in your ageing, says Cohen. The problem with that view of when ageing starts, he says, is that it can go back in time forever. Cohen thinks that ageing starts from the point at which egg and sperm meet \u2014 at conception.",
            "Others think that ageing starts on the day of birth. A few said it starts when puberty hits. Some consider that ageing begins only when the body stops developing, when a person reaches their twenties, or a few years later, when the body reaches peak performance, in the mid-twenties.",
            "Ultimately, Gladyshev says, the vast diversity of responses reflects the many unknowns in the field. Gladyshev expects rapid advances in efforts to define ageing, including the development of biomarkers to track biological age. \u201cIt\u2019s a time of opportunity.\u201d",
            "doi: https://doi.org/10.1038/d41586-024-03936-8"
        ]
    },
    {
        "URL": "https://www.nature.com/articles/d41586-024-03886-1",
        "title": "What\u2019s the secret to living to 100? Centenarian stem cells could offer clues",
        "content": [
            "*Marie_Hendrix_L_and_Gabrielle_Vaudremer,_100-year-old_Belgian_twins_celebrate_their_birthday_with_a_",
            "Credit: Nicolas Lambert/BELGA/AFP via Getty",
            "Scientists in Boston, Massachusetts have made reprogrammed stem cells from the blood of centenarians. They plan to share the cells with other researchers to better understand the factors that contribute to a long and healthy life. Early experiments are already providing insights on brain ageing.",
            "doi: https://doi.org/10.1038/d41586-024-03886-1"
        ]
    }
]